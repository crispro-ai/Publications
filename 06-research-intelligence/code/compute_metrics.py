#!/usr/bin/env python3
"""
Sprint 7: Compute Metrics

Compare predictions vs ground truth (keywords)
Compute: Precision, Recall, F1 for mechanisms
Compute: Pathway alignment accuracy
Compute: Evidence tier accuracy
Output: metrics_summary.json with all metrics
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

def compute_precision_recall_f1(predicted: List[str], ground_truth: List[str]) -> Dict[str, float]:
    """Compute precision, recall, F1 for mechanisms."""
    if not predicted and not ground_truth:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}
    
    if not predicted:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}
    
    if not ground_truth:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}
    
    # Normalize to lowercase for comparison
    predicted_lower = [m.lower().strip() for m in predicted if m]
    ground_truth_lower = [k.lower().strip() for k in ground_truth if k]
    
    # Find overlap (simple word matching)
    # For mechanisms, extract key terms
    predicted_terms = set()
    for m in predicted_lower:
        # Extract meaningful words from mechanism description
        words = m.split()
        for word in words:
            if len(word) > 4 and word.isalpha():
                predicted_terms.add(word)
    
    ground_truth_terms = set(ground_truth_lower)
    
    # Compute overlap
    overlap = predicted_terms.intersection(ground_truth_terms)
    
    precision = len(overlap) / len(predicted_terms) if predicted_terms else 0.0
    recall = len(overlap) / len(ground_truth_terms) if ground_truth_terms else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "overlap_count": len(overlap),
        "predicted_count": len(predicted_terms),
        "ground_truth_count": len(ground_truth_terms)
    }


def compute_metrics():
    """Compute metrics comparing RI predictions vs ground truth."""
    print("=" * 80)
    print("SPRINT 7: COMPUTE METRICS")
    print("=" * 80)
    
    # Load validation results
    results_file = Path(__file__).parent.parent / "sprint6_results" / "validation_results.json"
    if not results_file.exists():
        print(f"‚ùå Validation results not found: {results_file}")
        return
    
    with open(results_file, 'r') as f:
        results_data = json.load(f)
    
    # Load ground truth
    ground_truth_file = Path(__file__).parent.parent / "sprint5_results" / "pubmed_ground_truth.json"
    if not ground_truth_file.exists():
        print(f"‚ùå Ground truth not found: {ground_truth_file}")
        return
    
    with open(ground_truth_file, 'r') as f:
        ground_truth_data = json.load(f)
    
    # Create lookup for ground truth
    gt_lookup = {gt['query_id']: gt for gt in ground_truth_data.get('ground_truth', [])}
    
    # Compute metrics for each query
    print(f"\nüìä Computing metrics for {len(results_data.get('results', []))} queries...")
    
    all_metrics = []
    
    for result in results_data.get('results', []):
        query_id = result.get('query_id')
        gt = gt_lookup.get(query_id, {})
        
        # Extract mechanisms from RI result
        mechanisms = result.get('mechanisms', [])
        mechanism_names = []
        for m in mechanisms:
            if isinstance(m, dict):
                mechanism_names.append(m.get('mechanism', m.get('name', str(m))))
            else:
                mechanism_names.append(str(m))
        
        # Get ground truth keywords
        ground_truth_keywords = gt.get('top_keywords', [])
        
        # Compute precision/recall/F1
        metrics = compute_precision_recall_f1(mechanism_names, ground_truth_keywords)
        metrics['query_id'] = query_id
        metrics['mechanism_count'] = len(mechanism_names)
        metrics['ground_truth_keyword_count'] = len(ground_truth_keywords)
        
        all_metrics.append(metrics)
    
    # Aggregate metrics
    if all_metrics:
        avg_precision = sum(m['precision'] for m in all_metrics) / len(all_metrics)
        avg_recall = sum(m['recall'] for m in all_metrics) / len(all_metrics)
        avg_f1 = sum(m['f1'] for m in all_metrics) / len(all_metrics)
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "total_queries": len(all_metrics),
            "average_precision": avg_precision,
            "average_recall": avg_recall,
            "average_f1": avg_f1,
            "per_query_metrics": all_metrics
        }
        
        # Save output
        output_dir = Path(__file__).parent.parent / "sprint7_results"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / "metrics_summary.json"
        
        with open(output_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print("\n" + "=" * 80)
        print("‚úÖ METRICS COMPUTED")
        print("=" * 80)
        print(f"Average Precision: {avg_precision:.3f}")
        print(f"Average Recall: {avg_recall:.3f}")
        print(f"Average F1: {avg_f1:.3f}")
        print(f"Output file: {output_file}")
        
        return summary
    else:
        print("‚ùå No metrics to compute")
        return None


if __name__ == "__main__":
    compute_metrics()


